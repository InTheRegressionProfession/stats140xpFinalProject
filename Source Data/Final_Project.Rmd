---
title: "final_project"
author: "Sebastian Rivera-Chepetla"
date: "2023-03-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Analysis of numerical predictors first.

Correlation Matrix of numerical predictors & p-values for their correlations:    

```{R}
#install.packages("Hmisc")
library("Hmisc")

spotify = read.csv("songs_normalize 2.csv")
sum(is.na(spotify))
View(spotify)

# correlation matrix of numerical predictors
num_cols = unlist(lapply(spotify, is.numeric))
View(spotify[,num_cols])
cor(spotify[,num_cols])
```

The function rcorr() [in Hmisc package] can be used to compute the significance levels for pearson and spearman correlations. It returns both the correlation coefficients and the p-value of the correlation for all possible pairs of columns in the data table.

```{R}
res2 = rcorr(as.matrix(spotify[,num_cols]))
res2
```

# ANOVA:   

```{R}
anova = aov(danceability~.,data = spotify_numeric)
summary(anova)
```

```{R}
anova = aov(danceability~.-artist-year-song-artist,data = spotify)
summary(anova)
```

'duration_ms','energy', 'loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo' are signficant. However,'loudness' and 'instrumentalness' appear to be weak predictors. 

# The  matrix {m_mod} contains all significant correlations between numerical predictors.   

```{R}
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
res2 =rcorr(as.matrix(spotify[,num_cols]))
matrix_df = flattenCorrMatrix(res2$r, res2$P)
m_mod = matrix_df[matrix_df$p<0.05,]
r = m_mod$row=="danceability"
c = m_mod$column=="danceability"
# matrix mod contains all significant correlations between numerical predictors
View(m_mod)
```

Many numerical predictors seem to be significantly correlated with 'danceability'    

The following are all that are significantly correlated with 'danceability':    

```{R}
m_mod[r,]
```

```{R}
m_mod[c,]
```

The numerical predictor that are significantly correlated with danceability are: 'energy', 'mode', 'speechiness', 'acousticness', 'liveness', 'valence', 'tempo', 'danceability'. But the ones that are the the highest correlated with danceability are valence (0.40), and speechiness (~0.15). Compared to the results of our anova, which had all the same ones as significant predictors with the addition of 'loudness', and 'instrumentalness'. However, it is important to note that both of these additional predictors are just barely significant and their respective contributions are minimal (MeanSq 0.075 and 0.073 respectively indicating that they only help explain only a small portion of the variance)

Also, of these predictors, we need to check if they are correlated among themselves because that may introduce issues with multicollinearity.    

```{R}
res3 =rcorr(as.matrix(spotify[,c("energy", "mode", "speechiness", "acousticness", "liveness", "valence", "tempo")]))
matrix_df2 = flattenCorrMatrix(res3$r, res3$P)
m_mod2 = (matrix_df2[matrix_df2$p<0.05,])
r2 = m_mod2$row=="valence"
c2 = m_mod2$column=="valence"
r2.2 = m_mod2$row=="speechiness"
c2.2 = m_mod2$column == "speechiness"
```

As we can see, plenty of these numerical predictors are significantly correlated with each other, which can negatively impact their use in a model. However, we will now shift our focus to valence and liveness since they seemed to be the only potential relevant numerical predictors:    

```{R}
m_mod2[r2,]
m_mod2[c2,]
```


As we can see, 'speechiness' and 'valence' are significantly correlated with each other, yet their correlation is only about 0.07, thus, it may still be worth to consider using them. furthermore, valence is significantly correlated with 'energy', 'mode', and 'acousticness' with correlations of 0.3344,-0.0746 respectively.     

```{R}
m_mod2[r2.2,]
m_mod2[c2.2,]
```

Also, from the output above, 'speechiness' is significantly correlated with 'liveness', 'valence','tempo', albeit, very weakly correlated.       

Scatterplot between valence & danceability:         

```{R}
plot(x=spotify$valence,y=spotify$danceability)
```

Scatterplot between speechiness & danceability:    

```{R}
plot(x=spotify$speechiness,y=spotify$danceability)
```


```{R}
plot(x=spotify$danceability,y=spotify$valence)
plot(x=spotify$danceability,y=spotify$speechiness)
```

There is some noticeable clustering on the right and left ends of both scatter-plots, not entirely sure the source of this behavior (if we can find what causes this we could possibly generate new features depending on what we find), but the trends are very weak from a visual inspection.    


Turning all non-numeric variables to factors for easier use:       

```{R}
spotify[,!num_cols] = as.data.frame(lapply(spotify[,!num_cols], as.factor))
spotify_cat = spotify[,!num_cols]
spotify_numeric = spotify[,num_cols]
View(spotify_cat)
View(spotify_numeric)
```

# Analsysis of Categorical Predictors:    


1. mean populartiry separated by explicit status (yes or no):

```{R}
yes_explicit = spotify[as.logical(spotify$explicit),]
no_explicit  = spotify[!as.logical(spotify$explicit),]
cat("Explicit:",mean(yes_explicit$danceability),"",
    "Not Exlicit:",mean(no_explicit$danceability),"\n")
```

Q: significant difference in means or not?

```{R}
t.test(yes_explicit$danceability,no_explicit$danceability) # assuming unequal variances cause its safer
```

Despite what past code showed, the difference in mean popularity between explicit and non-explicit songs is actually significant, albeit close to being not significant which may indicate that the difference is not practically significant? 

# multiple boxplots split by explicit:

```{R}
#install.packages("ggplot2")
library("ggplot2")
ggplot(spotify, aes(x = explicit, y = danceability)) +
  geom_boxplot()
```

# Density Plots separated by artist type:    

```{R}
ggplot(spotify, aes(x = danceability, colour = explicit)) +
  geom_density()
```


Thus, the explicit predictor may not be that useful in predicting popularity.


*** Clustering may or may not be helpful in determining useful groupings that are associated with higher/lower mean danceability: ***

Running k-means clustering on numerical predictors:

```{R}
# Installing Packages
#install.packages("ClusterR")
#install.packages("cluster")
  
# Loading package
library(ClusterR)
library(cluster)

set.seed(005408206) # Setting seed
kmeans.re = kmeans(spotify_numeric, centers = 5, nstart = 10)
kmeans.re
```

A quick look at our results above shows that cluster 1 has the highest mean popularity of the 3 clusters, and its mean duration is also the largest across all 3 clusters, further supports the hypothesis that loinger songs may be more popular.


# Other Potential Ideas for Exploration: using genre as a blocking factor (and predictor):  

** Current variable has WAY too many levels, may be helpful to first start by grouping into broader categories. A quick look at the genre column reveals plenty of hip hop, pop, rock,rap, and other genres, yet many songs include several types of genres, how we form larger clusters of genres may affect how useful our new predictor for genre is for predicting popularity and thus would be worth investigating further. ** 

```{R}
length(levels(spotify$genre))
```



